{
 "metadata": {
  "name": "",
  "signature": "sha256:207af99fe89cd521227aacf5ee236a033dcf5a22054258771844dd8f7d6294a8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic Classification Example with the Iris Data Set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pylab as pl\n",
      "\n",
      "\n",
      "from sklearn import datasets\n",
      "\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Acquire the data: Flower Power"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the data\n",
      "# take a look at the datasets available:\n",
      "#datasets."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import built in iris data set\n",
      "iris = datasets.load_iris()\n",
      "print iris.data\n",
      "\n",
      "df = pd.DataFrame(iris.data)\n",
      "\n",
      "#the iris.data means that it'll take only the values rather than all the information that comes in the library such as dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'datasets' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-6e823d99222d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import built in iris data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# About the data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(iris.DESCR)\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Take a look\n",
      "df = pd.DataFrame(iris.data)\n",
      "df.columns=(iris.feature_names)\n",
      "df['Flower Type']=pd.Series(iris.target)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean - Its already pretty clean thanks to Sir Ronald Fisher\n",
      "![](http://upload.wikimedia.org/wikipedia/commons/thumb/4/46/R._A._Fischer.jpg/200px-R._A._Fischer.jpg)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Explore\n",
      "Our 3 Species:\n",
      "![](http://note.io/1iY4u5U)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.tools.plotting.scatter_matrix??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the pd.tools.plotting.scatter_matrix has a built in code that allows to create an entire matrix of histograms where it plots al lthe variables against each other"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.tools.plotting.scatter_matrix(df,alpha=.8,figsize=(15,15))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Subset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll focus on the Sepal\n",
      "#Take the first two features\n",
      "X = iris.data[:,:2]  \n",
      "Y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we are classifying what flower it is based on sepal length and width"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Background on Logistic Classification\n",
      "\n",
      "* Binary Logistic Regression on Each Class vs. the Others\n",
      "\n",
      "* Logistic regression measures the relationship between a categorical dependent variable and one or more independent variables, which are usually (but not necessarily) continuous, by using probability scores as the predicted values of the dependent variable\n",
      "\n",
      "* constrain the predictions of the model to the range [0,1] so that we can interpret them as probability estimates. In Logistic Regression, we use the logit function to clamp predictions from the range [\u2212\u221e,\u221e] to [0,1]:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-10, 10, 100)\n",
      "y = 1.0 / (1.0 + np.exp(-x))\n",
      "import matplotlib.pyplot as plt\n",
      "plt.plot(x, y, 'r-', label='logit')\n",
      "plt.legend(loc='lower right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The logistic function takes on values between 0 and 1, with input B0 + B1X and output F(x) (like a linear regression)\n",
      "\n",
      "* Multiple logistic regression has B0 + B1X + B2X2, etc.\n",
      "\n",
      "* Regression coefficients can be evaluated\n",
      "\n",
      "As compared with linear regression, we use the logit function to give an answer for all in the range 0 to 1:\n",
      "![](http://note.io/QfIYg6)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Resources\n",
      "* [Great background](http://melodi.ee.washington.edu/~halloj3/classification.pdf) from John Halloran on logistic regression vs naive bayes with examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Now lets classify some Irisses!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, datasets\n",
      "from sklearn.cross_validation import train_test_split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This pulls the linear regression model and cross validation from sklearn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Slice off a test set\n",
      "# Use train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This splits up the data sets in to 4 things- X_train, X_test, Y_train, Y_test aand we set the test size to be 25% of the data set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "logreg_fitter = linear_model.LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the parenthesse swe can provide attributes to do specific linear regressions on\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Logistic Regression\n",
      "\n",
      "logclf=logreg_fitter.fit(X_train, Y_train)\n",
      "\n",
      "#this fits the log linear regression to the training data\n",
      "# now this is a classifier based on sepal length and width\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Coefficients for each category\n",
      "logclf.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the first column is the y intercept, what is the 2nd column??"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logclf.score(X_test,Y_test)\n",
      "\n",
      "#this score is telling you how much of our test data came out accurately categorized\n",
      "#built in the scoring function is a prediction ..so it tests predictions against the real data\n",
      "\n",
      "#so what it needs: the predictor and the x values\n",
      "#so given these two things, can it predict the y values correctly? \n",
      "#then u are comparing the predicted Y values vs the test set Y values\n",
      "\n",
      "#what are the x and y here??\n",
      "\n",
      "#the Y values here are the classfications"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logclf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logclf.predict_proba(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use these to predict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the decision boundary. For that, we will assign a color to each\n",
      "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "h=.1\n",
      "#Plot Using only the training data\n",
      "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "\n",
      "# Z is predicting the category of the grid based on our prediction algorithm\n",
      "Z = logclf.predict(np.c_[xx.ravel(), yy.ravel()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "pl.figure(1, figsize=(15, 11))\n",
      "pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "this is the predicted values of Y (the sepal width given the sepal length) \n",
      "THe colors determine flower type"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the TRAINING points\n",
      "pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "pl.xlabel('Sepal length',size='xx-large')\n",
      "pl.ylabel('Sepal width',size='xx-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " # Plot the TESTING points\n",
      "pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "pl.xlabel('Sepal length')\n",
      "pl.ylabel('Sepal width')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combined\n",
      "makeplot_train(logclf)\n",
      "print \"Accuracy Score of the Trained Classifier:\",logclf.score(X_train,Y_train)\n",
      "\n",
      "makeplot_test(logclf)\n",
      "print \"Accuracy Score of the Trained Classifier on the Test Set:\",logclf.score(X_test,Y_test)\n",
      "\n",
      "makeplot_both(logclf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#So here we are seeing how the classifier plots the test and training set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#logistic classfier- asks a binary question: For any given value of X in class X is the probability that its a Y in classfier Y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Appendix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Plotting Functions\n",
      "_Note that these could be made much cleaner.  They should be transformed into a single plotting function along with variables for classification in put, test/train/both, multiplotting, etc. (maybe a good way to contribute to sklearn?)_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = .01 # step size in the mesh for visualizing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.pcolormesh?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PLOT JUST THE TRAINING SET\n",
      "def makeplot_train(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    #Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    \n",
      "    # Create a meshgrid -- see what happens when h is smaller...\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    \n",
      "    # Use classifier predictions for our point color\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot for our background \n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "    \n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "    pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "    \n",
      "     # Plot the TESTING points\n",
      "   # pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    #pl.xlabel('Sepal length')\n",
      "   # pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PLOT JUST THE TEST SET WITH TRAINING BOUNDARIES\n",
      "def makeplot_test(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    #Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "   # pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    #pl.xlabel('Sepal length')\n",
      "    #pl.ylabel('Sepal width')\n",
      "    \n",
      "    #Plot the TESTING points\n",
      "    pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='k', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def makeplot_both(classifier):\n",
      "        # Plot the decision boundary. For that, we will assign a color to each\n",
      "        # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "    \n",
      "    #Plot Using only the training data\n",
      "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
      "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "   \n",
      "    # Put the result into a color plot\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    pl.figure(1, figsize=(15, 11))\n",
      "    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n",
      "\n",
      "   \n",
      "    \n",
      "    # Plot the TRAINING points\n",
      "    pl.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, edgecolors='k', s=100  , cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "    \n",
      "     # Plot the TESTING points\n",
      "    pl.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, edgecolors='y', marker='v' ,s=100 ,cmap=pl.cm.Paired)\n",
      "    pl.xlabel('Sepal length')\n",
      "    pl.ylabel('Sepal width')\n",
      "\n",
      "    pl.xlim(xx.min(), xx.max())\n",
      "    pl.ylim(yy.min(), yy.max())\n",
      "    pl.xticks(())\n",
      "    pl.yticks(())\n",
      "    pl.title(classifier.__module__)\n",
      "    pl.size(5)\n",
      "\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Your turn\n",
      "\n",
      "## 1) Add in a classifier on the same data for decision trees\n",
      "\n",
      "## 2) Add in a classifier for naive bayes\n",
      "\n",
      "## 3) How do they all compare?\n",
      "\n",
      "## 4) Try it with test sizes of 3% and 40% -- how do they differ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25)\n",
      "\n",
      "from sklearn import tree\n",
      "clf2 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
      "clf2 = clf2.fit(X_train,Y_train)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn import metrics\n",
      "def measure_performance(X,Y,clf2, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
      "    Y_pred2=clf2.predict(X)\n",
      "    if show_accuracy:\n",
      "        print \"Accuracy:{0: .3f}\" .format(metrics.accuracy_score(Y,Y_pred2)), \"\\n\"\n",
      "        \n",
      "    \n",
      "    if show_classification_report:\n",
      "        print \"Classification report\"\n",
      "        print metrics.classification_report(Y,Y_pred2), \"\\n\"\n",
      "    \n",
      "    if show_confusion_matrix:\n",
      "        print \"Confusion Matrix\"\n",
      "        print metrics.confusion_matrix(Y,Y_pred2), \"\\n\"\n",
      "        \n",
      "    \n",
      "measure_performance(X_train,Y_train,clf2, show_classification_report=True, show_confusion_matrix=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'train_test_split' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-22-3e71e34d62ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.4)\n",
      "from sklearn import tree\n",
      "clf2 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
      "clf2 = clf2.fit(X_train,Y_train)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn import metrics\n",
      "def measure_performance(X,Y,clf2, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
      "    Y_pred2=clf2.predict(X)\n",
      "    if show_accuracy:\n",
      "        print \"Accuracy:{0: .3f}\" .format(metrics.accuracy_score(Y,Y_pred2)), \"\\n\"\n",
      "        \n",
      "    \n",
      "    if show_classification_report:\n",
      "        print \"Classification report\"\n",
      "        print metrics.classification_report(Y,Y_pred2), \"\\n\"\n",
      "    \n",
      "    if show_confusion_matrix:\n",
      "        print \"Confusion Matrix\"\n",
      "        print metrics.confusion_matrix(Y,Y_pred2), \"\\n\"\n",
      "        \n",
      "    \n",
      "measure_performance(X_train,Y_train,clf2, show_classification_report=True, show_confusion_matrix=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'train_test_split' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-17-93154c5cf799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#So the decision tree algo will overfit when u have less data but Naive Bayes or Logistic will do the opposite.\n",
      "#if you increase the node depth that will also overfit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.03)\n",
      "from sklearn import tree\n",
      "clf2 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
      "clf2 = clf2.fit(X_train,Y_train)\n",
      "\n",
      "\n",
      "\n",
      "from sklearn import metrics\n",
      "def measure_performance(X,Y,clf2, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
      "    Y_pred2=clf2.predict(X)\n",
      "    if show_accuracy:\n",
      "        print \"Accuracy:{0: .3f}\" .format(metrics.accuracy_score(Y,Y_pred2)), \"\\n\"\n",
      "        \n",
      "    \n",
      "    if show_classification_report:\n",
      "        print \"Classification report\"\n",
      "        print metrics.classification_report(Y,Y_pred2), \"\\n\"\n",
      "    \n",
      "    if show_confusion_matrix:\n",
      "        print \"Confusion Matrix\"\n",
      "        print metrics.confusion_matrix(Y,Y_pred2), \"\\n\"\n",
      "        \n",
      "    \n",
      "measure_performance(X_train,Y_train,clf2, show_classification_report=True, show_confusion_matrix=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'train_test_split' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-19-98da57ae0a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gnb = GaussianNB()\n",
      "bnb = BernoulliNB()\n",
      "lr = LogisticRegression()\n",
      "dt = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5)\n",
      "knn = KNeighborsClassifier()\n",
      "\n",
      "#1. Train your classifier on the Training data set\n",
      "gnb.fit(X_train, Y_train)\n",
      "bnb.fit(X_train, Y_train)\n",
      "lr.fit(X_train, Y_train)\n",
      "dt.fit(X_train, Y_train)\n",
      "knn.fit(X_train, Y_train)\n",
      "\n",
      "#2. Score the classifier using the test data set\n",
      "gnb_score = gnb.score(X_test, Y_test)\n",
      "bnb_score = bnb.score(X_test, Y_test)\n",
      "lr_score = lr.score(X_test, Y_test)\n",
      "dt_score = dt.score(X_test, Y_test)\n",
      "knn_score = knn.score(X_test, Y_test)\n",
      "#clf (classification) = gnb, bnb, lr\n",
      "\n",
      "#3. Print the scores\n",
      "print \"%0.2f%%\" %(gnb_score * 100.0)\n",
      "print \"%0.2f%%\" %(bnb_score * 100.0)\n",
      "print \"%0.2f%%\" %(lr_score * 100.0)\n",
      "print \"%0.2f%%\" %(dt_score * 100.0)\n",
      "print \"%0.2f%%\" %(gnb_score * 100.0)\n",
      "# the '%0.2f%%' is telling it to give me only 2 decimal points in the form of 0.01\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'X_train' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-448ba76373c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#1. Train your classifier on the Training data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Fit routine, that will train the model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}